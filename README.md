# BroadcastStream — Live Broadcast Streaming Microservice

> **Minimal, production-ready HLS microservice with ABR (Adaptive Bitrate), ~2s latency, and CDN support.**

```
Streamer (OBS/mobile) → RTMP → MediaMTX → FFmpeg ABR → LL-HLS → NGINX → CDN → Viewer
```

---

## Quick Start

**Prerequisites:** Docker Desktop, OBS Studio

```bash
# 1. Clone and start
git clone <repo> broadcaststream && cd broadcaststream
docker compose up --build

# 2. Configure OBS
#    Settings → Stream → Service: Custom
#    Server:     rtmp://localhost:1935/live
#    Stream Key: test

# 3. Start streaming in OBS, then open the viewer
open public/viewer.html
# OR visit http://localhost:8080/hls/test/master.m3u8 in VLC
```

**Ports:**

| Port   | Purpose                            |
| ------ | ---------------------------------- |
| `1935` | RTMP ingest (OBS → MediaMTX)       |
| `8080` | HLS HTTP (segments → viewer / CDN) |
| `4000` | REST API (health, stream list)     |
| `9997` | MediaMTX internal API (debugging)  |

---

## API Endpoints

```bash
GET  http://localhost:4000/health          # Liveness probe
GET  http://localhost:4000/streams         # All active streams
GET  http://localhost:4000/streams/:key    # Single stream info

# MediaMTX internal callbacks (do not call manually):
POST http://localhost:4000/webhook/on-publish
POST http://localhost:4000/webhook/on-unpublish
```

---

## Project Structure

```
broadcaststream/
├── docker-compose.yml      # 3-service stack: mediamtx + nginx + api
├── Dockerfile.api          # Multi-stage TS → Node.js API image
├── mediamtx.yml            # RTMP ingest + webhook + transcode trigger
├── transcode.sh            # FFmpeg ABR LL-HLS command (1 pass, 4 outputs)
├── nginx/
│   └── nginx.conf          # HLS HTTP server with CORS + cache headers
├── src/
│   ├── index.ts            # Express entry point (port 4000)
│   ├── types/index.ts      # Shared TypeScript types
│   ├── services/
│   │   └── streamTracker.ts  # In-memory stream registry
│   └── routes/
│       ├── health.ts       # GET /health
│       ├── streams.ts      # GET /streams, GET /streams/:key
│       └── webhooks.ts     # POST /webhook/on-publish|on-unpublish
└── public/
    └── viewer.html         # HLS.js test player (open in browser)
```

---

## Local Dev (without Docker)

Requires Homebrew + FFmpeg + MediaMTX:

```bash
# Install dependencies
brew install ffmpeg
brew install mediamtx   # or: go install github.com/bluenviron/mediamtx@latest

# Install node deps + compile TypeScript
npm install && npm run build

# Start MediaMTX (RTMP ingest)
mediamtx mediamtx.yml

# Start the API
npm start

# HLS output will be in /var/www/hls — create it if needed:
sudo mkdir -p /var/www/hls && sudo chmod 777 /var/www/hls

# Start OBS with Server: rtmp://localhost:1935/live  Key: test
# Open: public/viewer.html
```

---

## ABR Quality Levels

Generated by a single FFmpeg command (`transcode.sh`):

| Quality | Resolution | Video Bitrate | Audio | Use Case              |
| ------- | ---------- | ------------- | ----- | --------------------- |
| 1080p   | 1920×1080  | 4,500 kbps    | 192k  | Fast WiFi / desktop   |
| 720p    | 1280×720   | 2,500 kbps    | 128k  | Good broadband        |
| 480p    | 854×480    | 1,200 kbps    | 96k   | Average mobile        |
| 360p    | 640×360    | 600 kbps      | 64k   | Weak/congested mobile |

HLS.js automatically switches quality based on measured bandwidth (`abrEwmaDefaultEstimate`).

---

## Latency Deep Dive

### How latency adds up

```
Encoder → RTMP push → MediaMTX RTSP relay → FFmpeg encode → HLS segment write
          [~50ms]       [~30ms]               [~100ms]       [1s segment]
HLS.js playlist poll → segment fetch → decode + render
 [up to 1s]                [network]    [~50ms]
```

| Stage                          | Delay     | Optimization Applied                  |
| ------------------------------ | --------- | ------------------------------------- |
| RTMP ingest                    | ~50ms     | —                                     |
| RTSP relay (MediaMTX → FFmpeg) | ~30ms     | internal loopback                     |
| FFmpeg encode                  | ~100ms    | `-preset ultrafast -tune zerolatency` |
| HLS segment accumulation       | **1s**    | `hls_time 1` (vs default 6s)          |
| Playlist staleness             | up to 1s  | HLS.js polls every fragment           |
| Viewer buffer minimum          | ~2s       | `liveSyncDurationCount: 2`            |
| **Total (best case)**          | **~2.5s** |                                       |
| **Total (average)**            | **~3–4s** |                                       |

> Standard HLS with 6s segments → **8–14s latency**. This stack cuts that to **~2–4s** without anything exotic.

---

## CDN Integration (Cloudflare)

### Why it works

HLS consists of two file types with very different caching needs:

| File     | Extension | Cache Policy           | Reason                                      |
| -------- | --------- | ---------------------- | ------------------------------------------- |
| Playlist | `.m3u8`   | `no-cache` (NGINX)     | Changes every ~1s as new segments appear    |
| Segment  | `.ts`     | `max-age=3600` (NGINX) | Immutable once written — same bytes forever |

Cloudflare will cache `.ts` segments at its 300+ PoPs globally. The origin (your NGINX) only gets hit for `.m3u8` polls from ~1 viewer per CDN edge, not from every viewer.

### Setup

```
1. Point Cloudflare DNS: stream.yourdomain.com → your server IP
2. Set SSL: Full (strict)
3. Disable caching for *.m3u8:
   Page Rule: stream.yourdomain.com/hls/*.m3u8  → Cache Level: Bypass
4. Enable caching for *.ts (default Cloudflare behavior)
5. Update HLS_BASE_URL env var:
   HLS_BASE_URL=https://stream.yourdomain.com/hls
```

Viewer URL becomes: `https://stream.yourdomain.com/hls/{key}/master.m3u8`

---

## How Big Players Handle This at Scale

### YouTube Live

- **Ingest:** Proprietary RTMP to Google data centers (multiple ingestion points globally)
- **Transcoding:** Custom GPU transcoding fleet — generates 8 quality levels (144p–4K)
- **Delivery:** Dash + HLS from Google Global Cache (GGC) — essentially their own CDN co-located in ISPs
- **Latency mode:** "Ultra-low latency" is CMAF LL-HLS at ~2–4s, "Normal" is 20–30s for max stability
- **Key insight:** They absorb buffering at scale by accepting higher latency for most viewers

### Twitch

- **Ingest:** RTMP with custom `ingest.twitch.tv` fleet (anycast routing)
- **Transcoding:** NVIDIA GPU transcoders — partners get dedicated transcoding, others share
- **Delivery:** Proprietary VOD CDN + Akamai fallback for HLS
- **Latency:** "Low Latency Mode" is ~3–5s using Twitch's LSHS (low-latency segment-based HLS)
- **Key insight:** They store segments in RAM on origin, not disk — eliminates filesystem I/O latency

### Amazon IVS (Interactive Video Service)

- **Stack:** Fully managed re-implementation of the same RTMP → HLS pipeline
- **Latency:** 2–5s with their "ultra-low latency" (proprietary LL-HLS implementation)
- **Delivery:** CloudFront with aggressive segment caching
- **Key insight:** Their "quotas" (concurrent channels per account) are essentially GPU transcoding slot reservations

### What This Stack Borrows From Them

| Big Player Technique       | This Stack's Equivalent                                    |
| -------------------------- | ---------------------------------------------------------- |
| Separate ingest + delivery | MediaMTX (ingest) + NGINX (delivery) are separate services |
| Segment-level CDN caching  | NGINX cache headers + Cloudflare `immutable` on `.ts`      |
| ABR transcoding            | FFmpeg `filter_complex` single-pass 4-rendition output     |
| Low-latency HLS config     | `hls_time 1`, `liveSyncDurationCount: 2` in HLS.js         |
| Health check endpoints     | `/health` for load balancer + Docker                       |
| No-polling stream registry | MediaMTX webhook → in-memory Map                           |

### Where This Stack Differs (and what to add later)

| Scale Problem          | Current Gap                                     | Future Solution                                                             |
| ---------------------- | ----------------------------------------------- | --------------------------------------------------------------------------- |
| >1 concurrent stream   | Single FFmpeg process per stream, single server | Kubernetes + GPU node pool for transcoding                                  |
| >100k viewers          | Single NGINX origin                             | Cloudflare CDN already in plan                                              |
| Multi-region ingest    | Single RTMP endpoint                            | Anycast IP + GeoDNS to nearest ingest PoP                                   |
| Stream recording / VOD | None                                            | Write HLS segments to S3 simultaneously                                     |
| GPU transcoding        | CPU only                                        | Replace `libx264` with `h264_nvenc` (NVIDIA) or `h264_videotoolbox` (Apple) |

---

## Environment Variables

| Variable       | Default                     | Description                                                                   |
| -------------- | --------------------------- | ----------------------------------------------------------------------------- |
| `PORT`         | `4000`                      | Node.js API port                                                              |
| `HLS_BASE_URL` | `http://localhost:8080/hls` | Base URL for HLS playlist URLs returned by API (set to CDN URL in production) |
| `NODE_ENV`     | `production`                | Node environment                                                              |

---

## TypeScript Development

```bash
npm install          # Install deps
npm run dev          # Run with ts-node-dev (hot reload)
npm run build        # Compile to dist/
npm run type-check   # Check types without emitting
npm start            # Run compiled dist/index.js
```
